{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport torch\nfrom torch import nn, optim\nfrom torchvision import datasets, transforms, utils, models\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-15T22:58:38.209239Z","iopub.execute_input":"2022-05-15T22:58:38.209911Z","iopub.status.idle":"2022-05-15T22:58:40.144000Z","shell.execute_reply.started":"2022-05-15T22:58:38.209872Z","shell.execute_reply":"2022-05-15T22:58:40.143255Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device.type}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-15T22:59:04.152009Z","iopub.execute_input":"2022-05-15T22:59:04.152264Z","iopub.status.idle":"2022-05-15T22:59:04.211731Z","shell.execute_reply.started":"2022-05-15T22:59:04.152234Z","shell.execute_reply":"2022-05-15T22:59:04.210985Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Set up data\nDATA_DIR = \"../input/10-monkey-species/training/training\"\nIMAGE_SIZE = (128, 128)\nBATCH_SIZE = 32\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.ImageFolder(root=DATA_DIR, transform=data_transforms)\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=BATCH_SIZE, shuffle=True,\n#     sampler=train_sampler\n)\n        \n# Plot samples\nsample_batch = next(iter(data_loader))\nplt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\nplt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True), (1, 2, 0)));\n\nlen(data_loader) * BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2022-05-15T23:00:51.291139Z","iopub.execute_input":"2022-05-15T23:00:51.291432Z","iopub.status.idle":"2022-05-15T23:00:52.707840Z","shell.execute_reply.started":"2022-05-15T23:00:51.291403Z","shell.execute_reply":"2022-05-15T23:00:52.707197Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#create generator for GAN\nclass Generator(nn.Module):\n    \n    def __init__(self, LATENT_SIZE):\n        super(Generator, self).__init__()\n        \n        self.main = nn.Sequential(\n            \n            # input dim: [-1, LATENT_SIZE, 1, 1]\n            \n            nn.ConvTranspose2d(LATENT_SIZE, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # output dim: [-1, 1024, 4, 4]\n\n            nn.ConvTranspose2d(1024, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # output dim: [-1, 1024, 8, 8]\n\n            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # output dim: [-1, 512, 16, 16]\n\n            nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # output dim: [-1, 128, 32, 32]\n            \n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # output dim: [-1, 64, 64, 64]\n\n            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(3),\n            \n            # output dim: [-1, 3, 128, 128]\n            \n            nn.Tanh()\n            \n            # output dim: [-1, 3, 128, 128]\n        )\n        \n    def forward(self, input):\n        output = self.main(input)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-15T23:01:50.851563Z","iopub.execute_input":"2022-05-15T23:01:50.851835Z","iopub.status.idle":"2022-05-15T23:01:50.863183Z","shell.execute_reply.started":"2022-05-15T23:01:50.851809Z","shell.execute_reply":"2022-05-15T23:01:50.862183Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#create discriminator for GAN\nclass Discriminator(nn.Module):\n    \n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.main = nn.Sequential(\n        \n            # input dim: [-1, 3, 128, 128]\n            \n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # output dim: [-1, 64, 64, 64]\n\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # output dim: [-1, 64, 32, 32]\n\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # output dim: [-1, 128, 16, 16]\n\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # output dim: [-1, 256, 8, 8]\n\n            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            # output dim: [-1, 512, 4, 4]\n\n            nn.Conv2d(64, 1, kernel_size=4, stride=1, padding=0),\n            \n            # output dim: [-1, 1, 1, 1]\n\n            nn.Flatten(),\n            \n            # output dim: [-1]\n\n            nn.Sigmoid()\n            \n            # output dim: [-1]\n        )\n\n    def forward(self, input):\n        output = self.main(input)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-05-15T23:02:09.290923Z","iopub.execute_input":"2022-05-15T23:02:09.291232Z","iopub.status.idle":"2022-05-15T23:02:09.301880Z","shell.execute_reply.started":"2022-05-15T23:02:09.291183Z","shell.execute_reply":"2022-05-15T23:02:09.300935Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#initialize the weights\ndef weights_init(m):\n    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n        \nLATENT_SIZE = 50\nLR = 0.001\n\ngenerator = Generator(LATENT_SIZE)\ngenerator.apply(weights_init)\ngenerator.to(device)\ndiscriminator = Discriminator()\ndiscriminator.apply(weights_init)\ndiscriminator.to(device);\n\ncriterion = nn.BCELoss()\noptimizerG = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\nfixed_noise = torch.randn(BATCH_SIZE, LATENT_SIZE, 1, 1, device=device)  ","metadata":{"execution":{"iopub.status.busy":"2022-05-15T23:06:00.645048Z","iopub.execute_input":"2022-05-15T23:06:00.645310Z","iopub.status.idle":"2022-05-15T23:06:03.964641Z","shell.execute_reply.started":"2022-05-15T23:06:00.645283Z","shell.execute_reply":"2022-05-15T23:06:03.963870Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from statistics import mean\n#train the GAN model\nimg_list = []\nD_real_epoch, D_fake_epoch, loss_dis_epoch, loss_gen_epoch = [], [], [], []\n\nNUM_EPOCHS = 100\n\nprint('Training started:\\n')\n\nfor epoch in range(NUM_EPOCHS):\n    \n    D_real_iter, D_fake_iter, loss_dis_iter, loss_gen_iter = [], [], [], []\n    \n    for real_batch, _ in data_loader:\n\n        # STEP 1: train discriminator\n        # ==================================\n        # Train with real data\n        discriminator.zero_grad()\n        \n        real_batch = real_batch.to(device)\n        real_labels = torch.ones((real_batch.shape[0],), dtype=torch.float).to(device)\n        \n        output = discriminator(real_batch).view(-1)\n        loss_real = criterion(output, real_labels)\n        \n        # Iteration book-keeping\n        D_real_iter.append(output.mean().item())\n        \n        # Train with fake data\n        noise = torch.randn(real_batch.shape[0], LATENT_SIZE, 1, 1).to(device)\n        \n        fake_batch = generator(noise)\n        fake_labels = torch.zeros_like(real_labels)\n        \n        output = discriminator(fake_batch.detach()).view(-1)\n        loss_fake = criterion(output, fake_labels)\n        \n        # Update discriminator weights\n        loss_dis = loss_real + loss_fake\n        loss_dis.backward()\n        optimizerD.step()\n        \n        # Iteration book-keeping\n        loss_dis_iter.append(loss_dis.mean().item())\n        D_fake_iter.append(output.mean().item())\n        \n        # STEP 2: train generator\n        # ==================================\n        generator.zero_grad()\n        output = discriminator(fake_batch).view(-1)\n        loss_gen = criterion(output, real_labels)\n        loss_gen.backward()\n        \n        # Book-keeping\n        loss_gen_iter.append(loss_gen.mean().item())\n        \n        # Update generator weights and store loss\n        optimizerG.step()\n        \n    print(f\"Epoch ({epoch + 1}/{NUM_EPOCHS})\\t\",\n          f\"Loss_G: {mean(loss_gen_iter):.4f}\",\n          f\"Loss_D: {mean(loss_dis_iter):.4f}\\t\",\n          f\"D_real: {mean(D_real_iter):.4f}\",\n          f\"D_fake: {mean(D_fake_iter):.4f}\")\n    \n    # Epoch book-keeping\n    loss_gen_epoch.append(mean(loss_gen_iter))\n    loss_dis_epoch.append(mean(loss_dis_iter))\n    D_real_epoch.append(mean(D_real_iter))\n    D_fake_epoch.append(mean(D_fake_iter))\n    \n    # Keeping track of the evolution of a fixed noise latent vector\n    with torch.no_grad():\n        fake_images = generator(fixed_noise).detach().cpu()\n        img_list.append(utils.make_grid(fake_images, normalize=True, nrows=10))\n        \nprint(\"\\nTraining ended.\")","metadata":{"execution":{"iopub.status.busy":"2022-05-16T00:12:07.963614Z","iopub.execute_input":"2022-05-16T00:12:07.963880Z","iopub.status.idle":"2022-05-16T01:12:50.758505Z","shell.execute_reply.started":"2022-05-16T00:12:07.963852Z","shell.execute_reply":"2022-05-16T01:12:50.757763Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#training loss \nplt.plot(np.array(loss_gen_epoch), label='loss_gen')\nplt.plot(np.array(loss_dis_epoch), label='loss_dis')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-05-16T01:43:34.677878Z","iopub.execute_input":"2022-05-16T01:43:34.678387Z","iopub.status.idle":"2022-05-16T01:43:34.871839Z","shell.execute_reply.started":"2022-05-16T01:43:34.678333Z","shell.execute_reply":"2022-05-16T01:43:34.871157Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.array(D_real_epoch), label='D_real')\nplt.plot(np.array(D_fake_epoch), label='D_fake')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Probability\")\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-05-16T01:43:41.613153Z","iopub.execute_input":"2022-05-16T01:43:41.613428Z","iopub.status.idle":"2022-05-16T01:43:41.795521Z","shell.execute_reply.started":"2022-05-16T01:43:41.613396Z","shell.execute_reply":"2022-05-16T01:43:41.794699Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%capture\n\nfig = plt.figure(figsize=(10, 10))\nims = [[plt.imshow(np.transpose(i,(1, 2, 0)), animated=True)] for i in img_list[::10]]\nani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=2000, blit=True)\nani.save('GAN.gif', writer='imagemagick', fps=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T01:43:44.746762Z","iopub.execute_input":"2022-05-16T01:43:44.747474Z","iopub.status.idle":"2022-05-16T01:43:50.207264Z","shell.execute_reply.started":"2022-05-16T01:43:44.747440Z","shell.execute_reply":"2022-05-16T01:43:50.206445Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"HTML(ani.to_jshtml())","metadata":{"execution":{"iopub.status.busy":"2022-05-16T01:44:03.270830Z","iopub.execute_input":"2022-05-16T01:44:03.271107Z","iopub.status.idle":"2022-05-16T01:44:05.390840Z","shell.execute_reply.started":"2022-05-16T01:44:03.271074Z","shell.execute_reply":"2022-05-16T01:44:05.389439Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}